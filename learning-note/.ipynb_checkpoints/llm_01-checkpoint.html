<!DOCTYPE html>
<html>
<title>LLM Note</title>
<body>

<h1>Direct Preference Optimization (DPO)</h1>
<p>Direct Preference Optimization (DPO) is a fine-tuning method for aligning language models with human preferences. Instead of relying on reinforcement learning (like RLHF), DPO uses a supervised approach where the model is trained to rank a preferred output higher than a dispreferred one based on human feedback. It achieves this by directly optimizing a preference score using a contrastive loss function, which simplifies the training process while maintaining alignment quality.</p>

</body>
</html>